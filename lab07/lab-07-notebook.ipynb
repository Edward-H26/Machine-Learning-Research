{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 07: Credit Card Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "author: Qiran Hu\n",
    "date: October 25, 2024\n",
    "embed-resources: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report, we aim to develop a predictive model to detect fraudulent credit card transactions based on their characteristics. Every day, millions of credit card transactions occur globally while there is a significant number are fraudulent attempts by bad actors seeking unauthorized access to funds. Traditionally, banks have relied on manual reviews to flag suspicious activities. For example, a transaction exceeding a certain amount or occurring in a different country than the cardholder's residence might trigger an alert. However, these methods are increasingly insufficient due to the large volume of transactions and the sophisticated techniques employed by fraudsters.\n",
    "\n",
    "To address these challenges, we propose utilizing advanced machine learning models to analyze transaction data in real time. By training our model on historical transaction data labeled as fraudulent or genuine, we can capture complex patterns and correlations that are not easily discernible through manual analysis. \n",
    "\n",
    "Our goal is to develop an automated fraud detector that appropriately balances false positives and false negatives. Minimizing false positives is crucial to prevent legitimate transactions from being declined, which can lead to customer frustration and loss of trust. Conversely, reducing false negatives is essential to prevent fraudulent transactions from slipping through undetected, which can result in financial losses and damage to the bank's reputation. By deploying a machine learning model that adapts to new fraud patterns, we aim to enhance the bank's ability to protect its customers effectively. While our model significantly improves the detection of fraudulent transactions based on measurable attributes, it could have its own limitations. The model requires continuous updates and monitoring to adapt to evolving fraudulent behaviors.\n",
    "\n",
    "As one can see, by proactively identifying and preventing fraudulent activities, we can safeguard our customers' financial well being and uphold the integrity of our services. Ultimately, the development of this predictive model represents a significant achievement to minimize losses due to fraud and strengthen our position in the competitive banking industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import parallel_backend\n",
    "import seaborn as sns\n",
    "\n",
    "# data generation and loading\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.tree import export_text\n",
    "\n",
    "# basic classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# basic regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ensembles for classification\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# ensembles for regression\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "# model tuning and preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# model metrics\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    fbeta_score,\n",
    "    make_scorer,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "fraud = pd.read_parquet(\"https://cs307.org/lab-07/data/fraud.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC01</th>\n",
       "      <th>PC02</th>\n",
       "      <th>PC03</th>\n",
       "      <th>PC04</th>\n",
       "      <th>PC05</th>\n",
       "      <th>PC06</th>\n",
       "      <th>PC07</th>\n",
       "      <th>PC08</th>\n",
       "      <th>PC09</th>\n",
       "      <th>PC10</th>\n",
       "      <th>...</th>\n",
       "      <th>PC21</th>\n",
       "      <th>PC22</th>\n",
       "      <th>PC23</th>\n",
       "      <th>PC24</th>\n",
       "      <th>PC25</th>\n",
       "      <th>PC26</th>\n",
       "      <th>PC27</th>\n",
       "      <th>PC28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.702080</td>\n",
       "      <td>-3.315274</td>\n",
       "      <td>0.612866</td>\n",
       "      <td>2.414363</td>\n",
       "      <td>-1.884599</td>\n",
       "      <td>1.514794</td>\n",
       "      <td>0.131483</td>\n",
       "      <td>0.225408</td>\n",
       "      <td>1.568789</td>\n",
       "      <td>-0.869887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496013</td>\n",
       "      <td>-0.096228</td>\n",
       "      <td>-0.924270</td>\n",
       "      <td>-0.175346</td>\n",
       "      <td>0.206700</td>\n",
       "      <td>-0.272192</td>\n",
       "      <td>-0.046401</td>\n",
       "      <td>0.192446</td>\n",
       "      <td>927.61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.690580</td>\n",
       "      <td>1.286872</td>\n",
       "      <td>1.629988</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>0.061507</td>\n",
       "      <td>-0.546067</td>\n",
       "      <td>0.508168</td>\n",
       "      <td>-0.007774</td>\n",
       "      <td>0.773795</td>\n",
       "      <td>-0.212306</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.355172</td>\n",
       "      <td>-0.665155</td>\n",
       "      <td>-0.021777</td>\n",
       "      <td>0.254960</td>\n",
       "      <td>-0.185833</td>\n",
       "      <td>0.019074</td>\n",
       "      <td>0.347278</td>\n",
       "      <td>0.163613</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.969394</td>\n",
       "      <td>-0.253345</td>\n",
       "      <td>-1.611232</td>\n",
       "      <td>0.627206</td>\n",
       "      <td>0.429682</td>\n",
       "      <td>0.178193</td>\n",
       "      <td>-0.174938</td>\n",
       "      <td>-0.095031</td>\n",
       "      <td>1.111306</td>\n",
       "      <td>-0.654715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140268</td>\n",
       "      <td>0.839200</td>\n",
       "      <td>-0.180253</td>\n",
       "      <td>0.090653</td>\n",
       "      <td>0.375609</td>\n",
       "      <td>0.856089</td>\n",
       "      <td>-0.023358</td>\n",
       "      <td>-0.030589</td>\n",
       "      <td>49.32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.553788</td>\n",
       "      <td>1.435771</td>\n",
       "      <td>-0.791836</td>\n",
       "      <td>-0.512074</td>\n",
       "      <td>0.937930</td>\n",
       "      <td>-0.567430</td>\n",
       "      <td>0.683134</td>\n",
       "      <td>0.310608</td>\n",
       "      <td>-0.070914</td>\n",
       "      <td>-1.480662</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000774</td>\n",
       "      <td>0.267238</td>\n",
       "      <td>-0.383380</td>\n",
       "      <td>-1.063272</td>\n",
       "      <td>-0.012404</td>\n",
       "      <td>0.644840</td>\n",
       "      <td>0.257065</td>\n",
       "      <td>0.191257</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.503234</td>\n",
       "      <td>0.520610</td>\n",
       "      <td>0.591397</td>\n",
       "      <td>1.054720</td>\n",
       "      <td>1.306610</td>\n",
       "      <td>-1.392141</td>\n",
       "      <td>0.421473</td>\n",
       "      <td>-0.274220</td>\n",
       "      <td>-0.718293</td>\n",
       "      <td>-0.520577</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027699</td>\n",
       "      <td>0.122597</td>\n",
       "      <td>-0.096967</td>\n",
       "      <td>0.334576</td>\n",
       "      <td>-0.174797</td>\n",
       "      <td>0.446997</td>\n",
       "      <td>-0.311952</td>\n",
       "      <td>0.437133</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67840</th>\n",
       "      <td>-0.815702</td>\n",
       "      <td>1.250443</td>\n",
       "      <td>1.235399</td>\n",
       "      <td>0.798663</td>\n",
       "      <td>-0.104867</td>\n",
       "      <td>-0.198403</td>\n",
       "      <td>0.468914</td>\n",
       "      <td>0.187117</td>\n",
       "      <td>-0.404717</td>\n",
       "      <td>-0.090196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099021</td>\n",
       "      <td>0.270789</td>\n",
       "      <td>-0.066755</td>\n",
       "      <td>0.052279</td>\n",
       "      <td>-0.366833</td>\n",
       "      <td>-0.435550</td>\n",
       "      <td>-0.376964</td>\n",
       "      <td>-0.147660</td>\n",
       "      <td>19.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67841</th>\n",
       "      <td>-3.517229</td>\n",
       "      <td>3.326821</td>\n",
       "      <td>-3.590262</td>\n",
       "      <td>0.674769</td>\n",
       "      <td>-0.679266</td>\n",
       "      <td>-0.469516</td>\n",
       "      <td>-1.135362</td>\n",
       "      <td>2.778095</td>\n",
       "      <td>-2.404956</td>\n",
       "      <td>0.378914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455767</td>\n",
       "      <td>0.388102</td>\n",
       "      <td>0.268986</td>\n",
       "      <td>0.382692</td>\n",
       "      <td>-0.653335</td>\n",
       "      <td>2.192962</td>\n",
       "      <td>-0.953907</td>\n",
       "      <td>-0.137082</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67842</th>\n",
       "      <td>2.138450</td>\n",
       "      <td>-0.001148</td>\n",
       "      <td>-1.778731</td>\n",
       "      <td>0.076097</td>\n",
       "      <td>0.618824</td>\n",
       "      <td>-0.512793</td>\n",
       "      <td>0.206600</td>\n",
       "      <td>-0.280997</td>\n",
       "      <td>0.585802</td>\n",
       "      <td>-0.000395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.193403</td>\n",
       "      <td>0.708043</td>\n",
       "      <td>-0.060317</td>\n",
       "      <td>0.076972</td>\n",
       "      <td>0.398423</td>\n",
       "      <td>-0.099414</td>\n",
       "      <td>-0.023301</td>\n",
       "      <td>-0.058588</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67843</th>\n",
       "      <td>-2.639999</td>\n",
       "      <td>3.023083</td>\n",
       "      <td>-1.028799</td>\n",
       "      <td>3.095028</td>\n",
       "      <td>1.612154</td>\n",
       "      <td>1.002180</td>\n",
       "      <td>0.584766</td>\n",
       "      <td>0.545651</td>\n",
       "      <td>-1.380780</td>\n",
       "      <td>0.679444</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123887</td>\n",
       "      <td>-0.576202</td>\n",
       "      <td>0.136905</td>\n",
       "      <td>-1.426640</td>\n",
       "      <td>-0.822473</td>\n",
       "      <td>-0.427101</td>\n",
       "      <td>-2.163853</td>\n",
       "      <td>-0.553031</td>\n",
       "      <td>4.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67844</th>\n",
       "      <td>-1.340224</td>\n",
       "      <td>-0.099470</td>\n",
       "      <td>2.569205</td>\n",
       "      <td>-1.023849</td>\n",
       "      <td>0.266961</td>\n",
       "      <td>-0.992928</td>\n",
       "      <td>0.664708</td>\n",
       "      <td>-0.141592</td>\n",
       "      <td>0.075265</td>\n",
       "      <td>-0.986827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029485</td>\n",
       "      <td>-0.186024</td>\n",
       "      <td>-0.010211</td>\n",
       "      <td>0.359903</td>\n",
       "      <td>0.314488</td>\n",
       "      <td>0.083369</td>\n",
       "      <td>-0.235308</td>\n",
       "      <td>-0.149792</td>\n",
       "      <td>82.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67845 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           PC01      PC02      PC03      PC04      PC05      PC06      PC07  \\\n",
       "0     -0.702080 -3.315274  0.612866  2.414363 -1.884599  1.514794  0.131483   \n",
       "1     -0.690580  1.286872  1.629988  0.002150  0.061507 -0.546067  0.508168   \n",
       "2      1.969394 -0.253345 -1.611232  0.627206  0.429682  0.178193 -0.174938   \n",
       "3     -0.553788  1.435771 -0.791836 -0.512074  0.937930 -0.567430  0.683134   \n",
       "4     -1.503234  0.520610  0.591397  1.054720  1.306610 -1.392141  0.421473   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "67840 -0.815702  1.250443  1.235399  0.798663 -0.104867 -0.198403  0.468914   \n",
       "67841 -3.517229  3.326821 -3.590262  0.674769 -0.679266 -0.469516 -1.135362   \n",
       "67842  2.138450 -0.001148 -1.778731  0.076097  0.618824 -0.512793  0.206600   \n",
       "67843 -2.639999  3.023083 -1.028799  3.095028  1.612154  1.002180  0.584766   \n",
       "67844 -1.340224 -0.099470  2.569205 -1.023849  0.266961 -0.992928  0.664708   \n",
       "\n",
       "           PC08      PC09      PC10  ...      PC21      PC22      PC23  \\\n",
       "0      0.225408  1.568789 -0.869887  ...  0.496013 -0.096228 -0.924270   \n",
       "1     -0.007774  0.773795 -0.212306  ... -0.355172 -0.665155 -0.021777   \n",
       "2     -0.095031  1.111306 -0.654715  ...  0.140268  0.839200 -0.180253   \n",
       "3      0.310608 -0.070914 -1.480662  ... -0.000774  0.267238 -0.383380   \n",
       "4     -0.274220 -0.718293 -0.520577  ... -0.027699  0.122597 -0.096967   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "67840  0.187117 -0.404717 -0.090196  ...  0.099021  0.270789 -0.066755   \n",
       "67841  2.778095 -2.404956  0.378914  ...  0.455767  0.388102  0.268986   \n",
       "67842 -0.280997  0.585802 -0.000395  ...  0.193403  0.708043 -0.060317   \n",
       "67843  0.545651 -1.380780  0.679444  ... -0.123887 -0.576202  0.136905   \n",
       "67844 -0.141592  0.075265 -0.986827  ...  0.029485 -0.186024 -0.010211   \n",
       "\n",
       "           PC24      PC25      PC26      PC27      PC28  Amount  Fraud  \n",
       "0     -0.175346  0.206700 -0.272192 -0.046401  0.192446  927.61      0  \n",
       "1      0.254960 -0.185833  0.019074  0.347278  0.163613    3.57      0  \n",
       "2      0.090653  0.375609  0.856089 -0.023358 -0.030589   49.32      0  \n",
       "3     -1.063272 -0.012404  0.644840  0.257065  0.191257    1.41      0  \n",
       "4      0.334576 -0.174797  0.446997 -0.311952  0.437133    0.76      0  \n",
       "...         ...       ...       ...       ...       ...     ...    ...  \n",
       "67840  0.052279 -0.366833 -0.435550 -0.376964 -0.147660   19.99      0  \n",
       "67841  0.382692 -0.653335  2.192962 -0.953907 -0.137082    0.76      0  \n",
       "67842  0.076972  0.398423 -0.099414 -0.023301 -0.058588    1.00      0  \n",
       "67843 -1.426640 -0.822473 -0.427101 -2.163853 -0.553031    4.01      0  \n",
       "67844  0.359903  0.314488  0.083369 -0.235308 -0.149792   82.12      0  \n",
       "\n",
       "[67845 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "To achieve the goal of this research, we will need information on previous credit card transactions, including whether or not they were fraudulent. The necessary data is provided in the following files:\n",
    "\n",
    "Credit Card Fraud Data: fraud.parquet\n",
    "\n",
    "### Source\n",
    "The data for this research originally comes from Kaggle. Citations for the data can be found on Kaggle.\n",
    "\n",
    "Kaggle: Credit Card Fraud Detection\n",
    "A brief description of the target variable is given.\n",
    "\n",
    "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "Similarly, a brief description of the feature variables is given.\n",
    "\n",
    "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are ‘Time’ and ‘Amount’. Feature ‘Time’ contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature ‘Amount’ is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature ‘Class’ is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
    "\n",
    "We are providing a modified version of this data for this research.\n",
    "\n",
    "Modifications include:\n",
    "\n",
    "Removed the Time variable as it is misleading.\n",
    "Reduced the number of samples, while maintaining the number of fraudulent transactions.\n",
    "The class imbalance is reduced, but the target is still highly imbalanced.\n",
    "Withheld some data that will be considered the production data.\n",
    "Renamed the target variable from Class to Fraud.\n",
    "Renamed the PCA transformed variables.\n",
    "\n",
    "### Data Dictionary\n",
    "Each observation in the train, test, and (hidden) production data contains information about a particular credit card transaction.\n",
    "\n",
    "#### Response Variable\n",
    "**`Fraud`** - `[int64]` status of the transaction. 1 indicates a fraudulent transaction and 0 indicates not fraud, a genuine transaction.\n",
    "\n",
    "#### Features Variable\n",
    "**`Amount`** - `[float64]` amount (in dollars) of the transaction.\n",
    "\n",
    "**`PC01 - PC28`** - `[float64]` the 28 principal components that encode information such as location and type of purchase while preserving customer privacy.\n",
    "\n",
    "Principal Component Analysis (PCA) is a method that we will learn about later in the course. For now, know that it takes some number of features as inputs, and outputs either the same or fewer features, that retain most of the original information in the features. You can assume things like location and type of purchase were among the original input features. (Ever had a credit card transaction denied while traveling?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_train, fraud_test = train_test_split(\n",
    "    fraud,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=fraud[\"Fraud\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X and y for train\n",
    "X_train = fraud_train.drop(\"Fraud\", axis=1)\n",
    "y_train = fraud_train[\"Fraud\"]\n",
    "\n",
    "# create X and y for test\n",
    "X_test = fraud_test.drop(\"Fraud\", axis=1)\n",
    "y_test = fraud_test[\"Fraud\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54276"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary statistics\n",
    "fraud_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_train.isna().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC01</th>\n",
       "      <th>PC02</th>\n",
       "      <th>PC03</th>\n",
       "      <th>PC04</th>\n",
       "      <th>PC05</th>\n",
       "      <th>PC06</th>\n",
       "      <th>PC07</th>\n",
       "      <th>PC08</th>\n",
       "      <th>PC09</th>\n",
       "      <th>PC10</th>\n",
       "      <th>...</th>\n",
       "      <th>PC21</th>\n",
       "      <th>PC22</th>\n",
       "      <th>PC23</th>\n",
       "      <th>PC24</th>\n",
       "      <th>PC25</th>\n",
       "      <th>PC26</th>\n",
       "      <th>PC27</th>\n",
       "      <th>PC28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.005443</td>\n",
       "      <td>0.018480</td>\n",
       "      <td>-0.031372</td>\n",
       "      <td>0.017772</td>\n",
       "      <td>-0.009605</td>\n",
       "      <td>-0.009348</td>\n",
       "      <td>-0.018387</td>\n",
       "      <td>0.003198</td>\n",
       "      <td>-0.016539</td>\n",
       "      <td>-0.018124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004128</td>\n",
       "      <td>-0.001353</td>\n",
       "      <td>-0.002412</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>-0.001958</td>\n",
       "      <td>0.003177</td>\n",
       "      <td>-0.001147</td>\n",
       "      <td>88.197903</td>\n",
       "      <td>0.005804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.993976</td>\n",
       "      <td>1.662456</td>\n",
       "      <td>1.623235</td>\n",
       "      <td>1.457941</td>\n",
       "      <td>1.420379</td>\n",
       "      <td>1.335366</td>\n",
       "      <td>1.345778</td>\n",
       "      <td>1.258709</td>\n",
       "      <td>1.128743</td>\n",
       "      <td>1.199189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771537</td>\n",
       "      <td>0.731430</td>\n",
       "      <td>0.623645</td>\n",
       "      <td>0.606987</td>\n",
       "      <td>0.521288</td>\n",
       "      <td>0.481326</td>\n",
       "      <td>0.402607</td>\n",
       "      <td>0.314146</td>\n",
       "      <td>241.535617</td>\n",
       "      <td>0.075961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-37.558067</td>\n",
       "      <td>-39.818310</td>\n",
       "      <td>-30.177317</td>\n",
       "      <td>-5.263068</td>\n",
       "      <td>-40.427726</td>\n",
       "      <td>-19.996349</td>\n",
       "      <td>-43.557242</td>\n",
       "      <td>-41.044261</td>\n",
       "      <td>-13.434066</td>\n",
       "      <td>-24.588262</td>\n",
       "      <td>...</td>\n",
       "      <td>-22.797604</td>\n",
       "      <td>-8.887017</td>\n",
       "      <td>-32.828995</td>\n",
       "      <td>-2.822684</td>\n",
       "      <td>-8.696627</td>\n",
       "      <td>-1.778061</td>\n",
       "      <td>-9.390980</td>\n",
       "      <td>-8.364853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.922229</td>\n",
       "      <td>-0.589814</td>\n",
       "      <td>-0.912355</td>\n",
       "      <td>-0.850846</td>\n",
       "      <td>-0.692709</td>\n",
       "      <td>-0.775145</td>\n",
       "      <td>-0.558211</td>\n",
       "      <td>-0.209322</td>\n",
       "      <td>-0.665746</td>\n",
       "      <td>-0.537963</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227668</td>\n",
       "      <td>-0.543765</td>\n",
       "      <td>-0.161701</td>\n",
       "      <td>-0.352556</td>\n",
       "      <td>-0.316913</td>\n",
       "      <td>-0.328084</td>\n",
       "      <td>-0.071194</td>\n",
       "      <td>-0.052977</td>\n",
       "      <td>5.490000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.024920</td>\n",
       "      <td>0.069761</td>\n",
       "      <td>0.163381</td>\n",
       "      <td>-0.009252</td>\n",
       "      <td>-0.054553</td>\n",
       "      <td>-0.279010</td>\n",
       "      <td>0.036508</td>\n",
       "      <td>0.020547</td>\n",
       "      <td>-0.058311</td>\n",
       "      <td>-0.090411</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028828</td>\n",
       "      <td>0.008003</td>\n",
       "      <td>-0.011956</td>\n",
       "      <td>0.041341</td>\n",
       "      <td>0.018836</td>\n",
       "      <td>-0.053074</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.011206</td>\n",
       "      <td>21.690000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.317910</td>\n",
       "      <td>0.807316</td>\n",
       "      <td>1.014823</td>\n",
       "      <td>0.751716</td>\n",
       "      <td>0.612680</td>\n",
       "      <td>0.391895</td>\n",
       "      <td>0.568318</td>\n",
       "      <td>0.327378</td>\n",
       "      <td>0.591383</td>\n",
       "      <td>0.452907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187829</td>\n",
       "      <td>0.528112</td>\n",
       "      <td>0.147897</td>\n",
       "      <td>0.441985</td>\n",
       "      <td>0.352210</td>\n",
       "      <td>0.237455</td>\n",
       "      <td>0.090447</td>\n",
       "      <td>0.078497</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.454930</td>\n",
       "      <td>22.057729</td>\n",
       "      <td>4.226108</td>\n",
       "      <td>12.132323</td>\n",
       "      <td>28.762671</td>\n",
       "      <td>23.917837</td>\n",
       "      <td>44.054461</td>\n",
       "      <td>17.903574</td>\n",
       "      <td>9.125535</td>\n",
       "      <td>13.811758</td>\n",
       "      <td>...</td>\n",
       "      <td>27.202839</td>\n",
       "      <td>8.361985</td>\n",
       "      <td>19.002942</td>\n",
       "      <td>4.022866</td>\n",
       "      <td>4.513681</td>\n",
       "      <td>3.119295</td>\n",
       "      <td>10.507884</td>\n",
       "      <td>15.522649</td>\n",
       "      <td>10199.440000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               PC01          PC02          PC03          PC04          PC05  \\\n",
       "count  54276.000000  54276.000000  54276.000000  54276.000000  54276.000000   \n",
       "mean      -0.005443      0.018480     -0.031372      0.017772     -0.009605   \n",
       "std        1.993976      1.662456      1.623235      1.457941      1.420379   \n",
       "min      -37.558067    -39.818310    -30.177317     -5.263068    -40.427726   \n",
       "25%       -0.922229     -0.589814     -0.912355     -0.850846     -0.692709   \n",
       "50%        0.024920      0.069761      0.163381     -0.009252     -0.054553   \n",
       "75%        1.317910      0.807316      1.014823      0.751716      0.612680   \n",
       "max        2.454930     22.057729      4.226108     12.132323     28.762671   \n",
       "\n",
       "               PC06          PC07          PC08          PC09          PC10  \\\n",
       "count  54276.000000  54276.000000  54276.000000  54276.000000  54276.000000   \n",
       "mean      -0.009348     -0.018387      0.003198     -0.016539     -0.018124   \n",
       "std        1.335366      1.345778      1.258709      1.128743      1.199189   \n",
       "min      -19.996349    -43.557242    -41.044261    -13.434066    -24.588262   \n",
       "25%       -0.775145     -0.558211     -0.209322     -0.665746     -0.537963   \n",
       "50%       -0.279010      0.036508      0.020547     -0.058311     -0.090411   \n",
       "75%        0.391895      0.568318      0.327378      0.591383      0.452907   \n",
       "max       23.917837     44.054461     17.903574      9.125535     13.811758   \n",
       "\n",
       "       ...          PC21          PC22          PC23          PC24  \\\n",
       "count  ...  54276.000000  54276.000000  54276.000000  54276.000000   \n",
       "mean   ...      0.004128     -0.001353     -0.002412      0.001284   \n",
       "std    ...      0.771537      0.731430      0.623645      0.606987   \n",
       "min    ...    -22.797604     -8.887017    -32.828995     -2.822684   \n",
       "25%    ...     -0.227668     -0.543765     -0.161701     -0.352556   \n",
       "50%    ...     -0.028828      0.008003     -0.011956      0.041341   \n",
       "75%    ...      0.187829      0.528112      0.147897      0.441985   \n",
       "max    ...     27.202839      8.361985     19.002942      4.022866   \n",
       "\n",
       "               PC25          PC26          PC27          PC28        Amount  \\\n",
       "count  54276.000000  54276.000000  54276.000000  54276.000000  54276.000000   \n",
       "mean       0.001119     -0.001958      0.003177     -0.001147     88.197903   \n",
       "std        0.521288      0.481326      0.402607      0.314146    241.535617   \n",
       "min       -8.696627     -1.778061     -9.390980     -8.364853      0.000000   \n",
       "25%       -0.316913     -0.328084     -0.071194     -0.052977      5.490000   \n",
       "50%        0.018836     -0.053074      0.000985      0.011206     21.690000   \n",
       "75%        0.352210      0.237455      0.090447      0.078497     76.000000   \n",
       "max        4.513681      3.119295     10.507884     15.522649  10199.440000   \n",
       "\n",
       "              Fraud  \n",
       "count  54276.000000  \n",
       "mean       0.005804  \n",
       "std        0.075961  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53961"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fraud_train[fraud_train[\"Fraud\"] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9941963298695556"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fraud_train[fraud_train[\"Fraud\"] == 0]) / len(fraud_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fraud_train[fraud_train[\"Fraud\"] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005803670130444395"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fraud_train[fraud_train[\"Fraud\"] == 1]) / len(fraud_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC01</th>\n",
       "      <th>PC02</th>\n",
       "      <th>PC03</th>\n",
       "      <th>PC04</th>\n",
       "      <th>PC05</th>\n",
       "      <th>PC06</th>\n",
       "      <th>PC07</th>\n",
       "      <th>PC08</th>\n",
       "      <th>PC09</th>\n",
       "      <th>PC10</th>\n",
       "      <th>...</th>\n",
       "      <th>PC21</th>\n",
       "      <th>PC22</th>\n",
       "      <th>PC23</th>\n",
       "      <th>PC24</th>\n",
       "      <th>PC25</th>\n",
       "      <th>PC26</th>\n",
       "      <th>PC27</th>\n",
       "      <th>PC28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "      <td>54276.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.005443</td>\n",
       "      <td>0.018480</td>\n",
       "      <td>-0.031372</td>\n",
       "      <td>0.017772</td>\n",
       "      <td>-0.009605</td>\n",
       "      <td>-0.009348</td>\n",
       "      <td>-0.018387</td>\n",
       "      <td>0.003198</td>\n",
       "      <td>-0.016539</td>\n",
       "      <td>-0.018124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004128</td>\n",
       "      <td>-0.001353</td>\n",
       "      <td>-0.002412</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>-0.001958</td>\n",
       "      <td>0.003177</td>\n",
       "      <td>-0.001147</td>\n",
       "      <td>88.197903</td>\n",
       "      <td>0.005804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.993976</td>\n",
       "      <td>1.662456</td>\n",
       "      <td>1.623235</td>\n",
       "      <td>1.457941</td>\n",
       "      <td>1.420379</td>\n",
       "      <td>1.335366</td>\n",
       "      <td>1.345778</td>\n",
       "      <td>1.258709</td>\n",
       "      <td>1.128743</td>\n",
       "      <td>1.199189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771537</td>\n",
       "      <td>0.731430</td>\n",
       "      <td>0.623645</td>\n",
       "      <td>0.606987</td>\n",
       "      <td>0.521288</td>\n",
       "      <td>0.481326</td>\n",
       "      <td>0.402607</td>\n",
       "      <td>0.314146</td>\n",
       "      <td>241.535617</td>\n",
       "      <td>0.075961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-37.558067</td>\n",
       "      <td>-39.818310</td>\n",
       "      <td>-30.177317</td>\n",
       "      <td>-5.263068</td>\n",
       "      <td>-40.427726</td>\n",
       "      <td>-19.996349</td>\n",
       "      <td>-43.557242</td>\n",
       "      <td>-41.044261</td>\n",
       "      <td>-13.434066</td>\n",
       "      <td>-24.588262</td>\n",
       "      <td>...</td>\n",
       "      <td>-22.797604</td>\n",
       "      <td>-8.887017</td>\n",
       "      <td>-32.828995</td>\n",
       "      <td>-2.822684</td>\n",
       "      <td>-8.696627</td>\n",
       "      <td>-1.778061</td>\n",
       "      <td>-9.390980</td>\n",
       "      <td>-8.364853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.922229</td>\n",
       "      <td>-0.589814</td>\n",
       "      <td>-0.912355</td>\n",
       "      <td>-0.850846</td>\n",
       "      <td>-0.692709</td>\n",
       "      <td>-0.775145</td>\n",
       "      <td>-0.558211</td>\n",
       "      <td>-0.209322</td>\n",
       "      <td>-0.665746</td>\n",
       "      <td>-0.537963</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227668</td>\n",
       "      <td>-0.543765</td>\n",
       "      <td>-0.161701</td>\n",
       "      <td>-0.352556</td>\n",
       "      <td>-0.316913</td>\n",
       "      <td>-0.328084</td>\n",
       "      <td>-0.071194</td>\n",
       "      <td>-0.052977</td>\n",
       "      <td>5.490000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.024920</td>\n",
       "      <td>0.069761</td>\n",
       "      <td>0.163381</td>\n",
       "      <td>-0.009252</td>\n",
       "      <td>-0.054553</td>\n",
       "      <td>-0.279010</td>\n",
       "      <td>0.036508</td>\n",
       "      <td>0.020547</td>\n",
       "      <td>-0.058311</td>\n",
       "      <td>-0.090411</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028828</td>\n",
       "      <td>0.008003</td>\n",
       "      <td>-0.011956</td>\n",
       "      <td>0.041341</td>\n",
       "      <td>0.018836</td>\n",
       "      <td>-0.053074</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.011206</td>\n",
       "      <td>21.690000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.317910</td>\n",
       "      <td>0.807316</td>\n",
       "      <td>1.014823</td>\n",
       "      <td>0.751716</td>\n",
       "      <td>0.612680</td>\n",
       "      <td>0.391895</td>\n",
       "      <td>0.568318</td>\n",
       "      <td>0.327378</td>\n",
       "      <td>0.591383</td>\n",
       "      <td>0.452907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187829</td>\n",
       "      <td>0.528112</td>\n",
       "      <td>0.147897</td>\n",
       "      <td>0.441985</td>\n",
       "      <td>0.352210</td>\n",
       "      <td>0.237455</td>\n",
       "      <td>0.090447</td>\n",
       "      <td>0.078497</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.454930</td>\n",
       "      <td>22.057729</td>\n",
       "      <td>4.226108</td>\n",
       "      <td>12.132323</td>\n",
       "      <td>28.762671</td>\n",
       "      <td>23.917837</td>\n",
       "      <td>44.054461</td>\n",
       "      <td>17.903574</td>\n",
       "      <td>9.125535</td>\n",
       "      <td>13.811758</td>\n",
       "      <td>...</td>\n",
       "      <td>27.202839</td>\n",
       "      <td>8.361985</td>\n",
       "      <td>19.002942</td>\n",
       "      <td>4.022866</td>\n",
       "      <td>4.513681</td>\n",
       "      <td>3.119295</td>\n",
       "      <td>10.507884</td>\n",
       "      <td>15.522649</td>\n",
       "      <td>10199.440000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               PC01          PC02          PC03          PC04          PC05  \\\n",
       "count  54276.000000  54276.000000  54276.000000  54276.000000  54276.000000   \n",
       "mean      -0.005443      0.018480     -0.031372      0.017772     -0.009605   \n",
       "std        1.993976      1.662456      1.623235      1.457941      1.420379   \n",
       "min      -37.558067    -39.818310    -30.177317     -5.263068    -40.427726   \n",
       "25%       -0.922229     -0.589814     -0.912355     -0.850846     -0.692709   \n",
       "50%        0.024920      0.069761      0.163381     -0.009252     -0.054553   \n",
       "75%        1.317910      0.807316      1.014823      0.751716      0.612680   \n",
       "max        2.454930     22.057729      4.226108     12.132323     28.762671   \n",
       "\n",
       "               PC06          PC07          PC08          PC09          PC10  \\\n",
       "count  54276.000000  54276.000000  54276.000000  54276.000000  54276.000000   \n",
       "mean      -0.009348     -0.018387      0.003198     -0.016539     -0.018124   \n",
       "std        1.335366      1.345778      1.258709      1.128743      1.199189   \n",
       "min      -19.996349    -43.557242    -41.044261    -13.434066    -24.588262   \n",
       "25%       -0.775145     -0.558211     -0.209322     -0.665746     -0.537963   \n",
       "50%       -0.279010      0.036508      0.020547     -0.058311     -0.090411   \n",
       "75%        0.391895      0.568318      0.327378      0.591383      0.452907   \n",
       "max       23.917837     44.054461     17.903574      9.125535     13.811758   \n",
       "\n",
       "       ...          PC21          PC22          PC23          PC24  \\\n",
       "count  ...  54276.000000  54276.000000  54276.000000  54276.000000   \n",
       "mean   ...      0.004128     -0.001353     -0.002412      0.001284   \n",
       "std    ...      0.771537      0.731430      0.623645      0.606987   \n",
       "min    ...    -22.797604     -8.887017    -32.828995     -2.822684   \n",
       "25%    ...     -0.227668     -0.543765     -0.161701     -0.352556   \n",
       "50%    ...     -0.028828      0.008003     -0.011956      0.041341   \n",
       "75%    ...      0.187829      0.528112      0.147897      0.441985   \n",
       "max    ...     27.202839      8.361985     19.002942      4.022866   \n",
       "\n",
       "               PC25          PC26          PC27          PC28        Amount  \\\n",
       "count  54276.000000  54276.000000  54276.000000  54276.000000  54276.000000   \n",
       "mean       0.001119     -0.001958      0.003177     -0.001147     88.197903   \n",
       "std        0.521288      0.481326      0.402607      0.314146    241.535617   \n",
       "min       -8.696627     -1.778061     -9.390980     -8.364853      0.000000   \n",
       "25%       -0.316913     -0.328084     -0.071194     -0.052977      5.490000   \n",
       "50%        0.018836     -0.053074      0.000985      0.011206     21.690000   \n",
       "75%        0.352210      0.237455      0.090447      0.078497     76.000000   \n",
       "max        4.513681      3.119295     10.507884     15.522649  10199.440000   \n",
       "\n",
       "              Fraud  \n",
       "count  54276.000000  \n",
       "mean       0.005804  \n",
       "std        0.075961  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = fraud_train\n",
    "a.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC01</th>\n",
       "      <th>PC02</th>\n",
       "      <th>PC03</th>\n",
       "      <th>PC04</th>\n",
       "      <th>PC05</th>\n",
       "      <th>PC06</th>\n",
       "      <th>PC07</th>\n",
       "      <th>PC08</th>\n",
       "      <th>PC09</th>\n",
       "      <th>PC10</th>\n",
       "      <th>...</th>\n",
       "      <th>PC21</th>\n",
       "      <th>PC22</th>\n",
       "      <th>PC23</th>\n",
       "      <th>PC24</th>\n",
       "      <th>PC25</th>\n",
       "      <th>PC26</th>\n",
       "      <th>PC27</th>\n",
       "      <th>PC28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>53961.000000</td>\n",
       "      <td>53961.000000</td>\n",
       "      <td>53961.000000</td>\n",
       "      <td>53961.000000</td>\n",
       "      <td>53961.000000</td>\n",
       "      <td>53961.000000</td>\n",
       "      <td>53961.000000</td>\n",
       "      <td>53961.000000</td>\n",
       "      <td>53961.000000</td>\n",
       "      <td>53961.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>53961.000000</td>\n",
       "      <td>53961.000000</td>\n",
       "      <td>53961.000000</td>\n",
       "      <td>53961.000000</td>\n",
       "      <td>53961.000000</td>\n",
       "      <td>53961.000000</td>\n",
       "      <td>53961.000000</td>\n",
       "      <td>53961.000000</td>\n",
       "      <td>53961.000000</td>\n",
       "      <td>53961.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.020248</td>\n",
       "      <td>-0.002301</td>\n",
       "      <td>0.008233</td>\n",
       "      <td>-0.008805</td>\n",
       "      <td>0.007156</td>\n",
       "      <td>-0.001725</td>\n",
       "      <td>0.013265</td>\n",
       "      <td>0.002936</td>\n",
       "      <td>-0.001540</td>\n",
       "      <td>0.014976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001196</td>\n",
       "      <td>-0.001327</td>\n",
       "      <td>-0.003133</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>-0.002467</td>\n",
       "      <td>0.001926</td>\n",
       "      <td>-0.001634</td>\n",
       "      <td>88.065104</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.912630</td>\n",
       "      <td>1.613970</td>\n",
       "      <td>1.455933</td>\n",
       "      <td>1.401877</td>\n",
       "      <td>1.356808</td>\n",
       "      <td>1.327216</td>\n",
       "      <td>1.163938</td>\n",
       "      <td>1.129916</td>\n",
       "      <td>1.097447</td>\n",
       "      <td>1.057875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.687133</td>\n",
       "      <td>0.722055</td>\n",
       "      <td>0.619411</td>\n",
       "      <td>0.607439</td>\n",
       "      <td>0.519968</td>\n",
       "      <td>0.481234</td>\n",
       "      <td>0.389812</td>\n",
       "      <td>0.312010</td>\n",
       "      <td>241.451144</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-37.558067</td>\n",
       "      <td>-39.818310</td>\n",
       "      <td>-27.747084</td>\n",
       "      <td>-5.263068</td>\n",
       "      <td>-40.427726</td>\n",
       "      <td>-19.996349</td>\n",
       "      <td>-19.083907</td>\n",
       "      <td>-34.535000</td>\n",
       "      <td>-4.658019</td>\n",
       "      <td>-10.656148</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.640785</td>\n",
       "      <td>-8.454599</td>\n",
       "      <td>-32.828995</td>\n",
       "      <td>-2.822684</td>\n",
       "      <td>-8.696627</td>\n",
       "      <td>-1.778061</td>\n",
       "      <td>-9.390980</td>\n",
       "      <td>-8.364853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.913161</td>\n",
       "      <td>-0.593659</td>\n",
       "      <td>-0.891704</td>\n",
       "      <td>-0.856323</td>\n",
       "      <td>-0.686080</td>\n",
       "      <td>-0.769775</td>\n",
       "      <td>-0.549787</td>\n",
       "      <td>-0.209487</td>\n",
       "      <td>-0.656590</td>\n",
       "      <td>-0.530080</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228092</td>\n",
       "      <td>-0.544006</td>\n",
       "      <td>-0.161136</td>\n",
       "      <td>-0.351849</td>\n",
       "      <td>-0.316770</td>\n",
       "      <td>-0.328373</td>\n",
       "      <td>-0.071220</td>\n",
       "      <td>-0.052951</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.032412</td>\n",
       "      <td>0.063483</td>\n",
       "      <td>0.169792</td>\n",
       "      <td>-0.018396</td>\n",
       "      <td>-0.051807</td>\n",
       "      <td>-0.275778</td>\n",
       "      <td>0.040079</td>\n",
       "      <td>0.019397</td>\n",
       "      <td>-0.053885</td>\n",
       "      <td>-0.087319</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029987</td>\n",
       "      <td>0.007817</td>\n",
       "      <td>-0.011842</td>\n",
       "      <td>0.041768</td>\n",
       "      <td>0.018488</td>\n",
       "      <td>-0.053729</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.011009</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.320862</td>\n",
       "      <td>0.796633</td>\n",
       "      <td>1.020624</td>\n",
       "      <td>0.732937</td>\n",
       "      <td>0.614121</td>\n",
       "      <td>0.394898</td>\n",
       "      <td>0.570578</td>\n",
       "      <td>0.323435</td>\n",
       "      <td>0.594950</td>\n",
       "      <td>0.456970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185045</td>\n",
       "      <td>0.527411</td>\n",
       "      <td>0.147480</td>\n",
       "      <td>0.442882</td>\n",
       "      <td>0.351896</td>\n",
       "      <td>0.236337</td>\n",
       "      <td>0.088888</td>\n",
       "      <td>0.077432</td>\n",
       "      <td>75.970000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.454930</td>\n",
       "      <td>14.845545</td>\n",
       "      <td>4.226108</td>\n",
       "      <td>12.132323</td>\n",
       "      <td>28.762671</td>\n",
       "      <td>23.917837</td>\n",
       "      <td>44.054461</td>\n",
       "      <td>17.573712</td>\n",
       "      <td>9.125535</td>\n",
       "      <td>13.811758</td>\n",
       "      <td>...</td>\n",
       "      <td>22.580675</td>\n",
       "      <td>6.090514</td>\n",
       "      <td>19.002942</td>\n",
       "      <td>4.022866</td>\n",
       "      <td>4.513681</td>\n",
       "      <td>3.119295</td>\n",
       "      <td>10.507884</td>\n",
       "      <td>15.522649</td>\n",
       "      <td>10199.440000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               PC01          PC02          PC03          PC04          PC05  \\\n",
       "count  53961.000000  53961.000000  53961.000000  53961.000000  53961.000000   \n",
       "mean       0.020248     -0.002301      0.008233     -0.008805      0.007156   \n",
       "std        1.912630      1.613970      1.455933      1.401877      1.356808   \n",
       "min      -37.558067    -39.818310    -27.747084     -5.263068    -40.427726   \n",
       "25%       -0.913161     -0.593659     -0.891704     -0.856323     -0.686080   \n",
       "50%        0.032412      0.063483      0.169792     -0.018396     -0.051807   \n",
       "75%        1.320862      0.796633      1.020624      0.732937      0.614121   \n",
       "max        2.454930     14.845545      4.226108     12.132323     28.762671   \n",
       "\n",
       "               PC06          PC07          PC08          PC09          PC10  \\\n",
       "count  53961.000000  53961.000000  53961.000000  53961.000000  53961.000000   \n",
       "mean      -0.001725      0.013265      0.002936     -0.001540      0.014976   \n",
       "std        1.327216      1.163938      1.129916      1.097447      1.057875   \n",
       "min      -19.996349    -19.083907    -34.535000     -4.658019    -10.656148   \n",
       "25%       -0.769775     -0.549787     -0.209487     -0.656590     -0.530080   \n",
       "50%       -0.275778      0.040079      0.019397     -0.053885     -0.087319   \n",
       "75%        0.394898      0.570578      0.323435      0.594950      0.456970   \n",
       "max       23.917837     44.054461     17.573712      9.125535     13.811758   \n",
       "\n",
       "       ...          PC21          PC22          PC23          PC24  \\\n",
       "count  ...  53961.000000  53961.000000  53961.000000  53961.000000   \n",
       "mean   ...     -0.001196     -0.001327     -0.003133      0.002055   \n",
       "std    ...      0.687133      0.722055      0.619411      0.607439   \n",
       "min    ...    -16.640785     -8.454599    -32.828995     -2.822684   \n",
       "25%    ...     -0.228092     -0.544006     -0.161136     -0.351849   \n",
       "50%    ...     -0.029987      0.007817     -0.011842      0.041768   \n",
       "75%    ...      0.185045      0.527411      0.147480      0.442882   \n",
       "max    ...     22.580675      6.090514     19.002942      4.022866   \n",
       "\n",
       "               PC25          PC26          PC27          PC28        Amount  \\\n",
       "count  53961.000000  53961.000000  53961.000000  53961.000000  53961.000000   \n",
       "mean       0.000950     -0.002467      0.001926     -0.001634     88.065104   \n",
       "std        0.519968      0.481234      0.389812      0.312010    241.451144   \n",
       "min       -8.696627     -1.778061     -9.390980     -8.364853      0.000000   \n",
       "25%       -0.316770     -0.328373     -0.071220     -0.052951      5.500000   \n",
       "50%        0.018488     -0.053729      0.000562      0.011009     21.800000   \n",
       "75%        0.351896      0.236337      0.088888      0.077432     75.970000   \n",
       "max        4.513681      3.119295     10.507884     15.522649  10199.440000   \n",
       "\n",
       "         Fraud  \n",
       "count  53961.0  \n",
       "mean       0.0  \n",
       "std        0.0  \n",
       "min        0.0  \n",
       "25%        0.0  \n",
       "50%        0.0  \n",
       "75%        0.0  \n",
       "max        0.0  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = fraud_train[fraud_train[\"Fraud\"] == 0]\n",
    "b.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC01</th>\n",
       "      <th>PC02</th>\n",
       "      <th>PC03</th>\n",
       "      <th>PC04</th>\n",
       "      <th>PC05</th>\n",
       "      <th>PC06</th>\n",
       "      <th>PC07</th>\n",
       "      <th>PC08</th>\n",
       "      <th>PC09</th>\n",
       "      <th>PC10</th>\n",
       "      <th>...</th>\n",
       "      <th>PC21</th>\n",
       "      <th>PC22</th>\n",
       "      <th>PC23</th>\n",
       "      <th>PC24</th>\n",
       "      <th>PC25</th>\n",
       "      <th>PC26</th>\n",
       "      <th>PC27</th>\n",
       "      <th>PC28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>315.000000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>315.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-4.406446</td>\n",
       "      <td>3.578318</td>\n",
       "      <td>-6.815934</td>\n",
       "      <td>4.570541</td>\n",
       "      <td>-2.880705</td>\n",
       "      <td>-1.315083</td>\n",
       "      <td>-5.440562</td>\n",
       "      <td>0.048049</td>\n",
       "      <td>-2.585855</td>\n",
       "      <td>-5.688388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.916146</td>\n",
       "      <td>-0.005732</td>\n",
       "      <td>0.120946</td>\n",
       "      <td>-0.130766</td>\n",
       "      <td>0.030085</td>\n",
       "      <td>0.085179</td>\n",
       "      <td>0.217511</td>\n",
       "      <td>0.082284</td>\n",
       "      <td>110.947016</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.249443</td>\n",
       "      <td>4.157588</td>\n",
       "      <td>6.687623</td>\n",
       "      <td>2.961478</td>\n",
       "      <td>4.903586</td>\n",
       "      <td>1.948734</td>\n",
       "      <td>7.111707</td>\n",
       "      <td>7.379311</td>\n",
       "      <td>2.567025</td>\n",
       "      <td>4.879710</td>\n",
       "      <td>...</td>\n",
       "      <td>4.573313</td>\n",
       "      <td>1.696558</td>\n",
       "      <td>1.131018</td>\n",
       "      <td>0.507588</td>\n",
       "      <td>0.712773</td>\n",
       "      <td>0.489928</td>\n",
       "      <td>1.363315</td>\n",
       "      <td>0.567332</td>\n",
       "      <td>254.978960</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-28.709229</td>\n",
       "      <td>-8.402154</td>\n",
       "      <td>-30.177317</td>\n",
       "      <td>-1.313275</td>\n",
       "      <td>-20.087878</td>\n",
       "      <td>-6.406267</td>\n",
       "      <td>-43.557242</td>\n",
       "      <td>-41.044261</td>\n",
       "      <td>-13.434066</td>\n",
       "      <td>-24.588262</td>\n",
       "      <td>...</td>\n",
       "      <td>-22.797604</td>\n",
       "      <td>-8.887017</td>\n",
       "      <td>-4.908301</td>\n",
       "      <td>-1.386745</td>\n",
       "      <td>-2.419446</td>\n",
       "      <td>-1.149923</td>\n",
       "      <td>-7.263482</td>\n",
       "      <td>-1.869290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-5.520077</td>\n",
       "      <td>1.290288</td>\n",
       "      <td>-8.582162</td>\n",
       "      <td>2.351424</td>\n",
       "      <td>-4.625915</td>\n",
       "      <td>-2.487808</td>\n",
       "      <td>-7.723720</td>\n",
       "      <td>-0.114607</td>\n",
       "      <td>-3.928330</td>\n",
       "      <td>-7.550001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131593</td>\n",
       "      <td>-0.518950</td>\n",
       "      <td>-0.320403</td>\n",
       "      <td>-0.468859</td>\n",
       "      <td>-0.332491</td>\n",
       "      <td>-0.246697</td>\n",
       "      <td>-0.004432</td>\n",
       "      <td>-0.086931</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-2.250535</td>\n",
       "      <td>2.726800</td>\n",
       "      <td>-5.076070</td>\n",
       "      <td>4.127549</td>\n",
       "      <td>-1.507035</td>\n",
       "      <td>-1.463272</td>\n",
       "      <td>-2.986028</td>\n",
       "      <td>0.643230</td>\n",
       "      <td>-2.099049</td>\n",
       "      <td>-4.592390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.587728</td>\n",
       "      <td>0.083353</td>\n",
       "      <td>-0.048155</td>\n",
       "      <td>-0.145493</td>\n",
       "      <td>0.050678</td>\n",
       "      <td>0.048935</td>\n",
       "      <td>0.421300</td>\n",
       "      <td>0.160769</td>\n",
       "      <td>6.990000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-0.436659</td>\n",
       "      <td>4.845798</td>\n",
       "      <td>-2.247442</td>\n",
       "      <td>6.348445</td>\n",
       "      <td>0.231410</td>\n",
       "      <td>-0.491548</td>\n",
       "      <td>-1.202573</td>\n",
       "      <td>1.717880</td>\n",
       "      <td>-0.770367</td>\n",
       "      <td>-2.733474</td>\n",
       "      <td>...</td>\n",
       "      <td>1.193791</td>\n",
       "      <td>0.615344</td>\n",
       "      <td>0.319270</td>\n",
       "      <td>0.256022</td>\n",
       "      <td>0.402808</td>\n",
       "      <td>0.430788</td>\n",
       "      <td>0.856969</td>\n",
       "      <td>0.397953</td>\n",
       "      <td>99.990000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.132386</td>\n",
       "      <td>22.057729</td>\n",
       "      <td>2.250210</td>\n",
       "      <td>11.927512</td>\n",
       "      <td>9.880564</td>\n",
       "      <td>6.474115</td>\n",
       "      <td>5.431271</td>\n",
       "      <td>17.903574</td>\n",
       "      <td>3.353525</td>\n",
       "      <td>4.031435</td>\n",
       "      <td>...</td>\n",
       "      <td>27.202839</td>\n",
       "      <td>8.361985</td>\n",
       "      <td>5.466230</td>\n",
       "      <td>1.091435</td>\n",
       "      <td>2.024388</td>\n",
       "      <td>2.745261</td>\n",
       "      <td>2.352333</td>\n",
       "      <td>1.521218</td>\n",
       "      <td>2125.870000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             PC01        PC02        PC03        PC04        PC05        PC06  \\\n",
       "count  315.000000  315.000000  315.000000  315.000000  315.000000  315.000000   \n",
       "mean    -4.406446    3.578318   -6.815934    4.570541   -2.880705   -1.315083   \n",
       "std      6.249443    4.157588    6.687623    2.961478    4.903586    1.948734   \n",
       "min    -28.709229   -8.402154  -30.177317   -1.313275  -20.087878   -6.406267   \n",
       "25%     -5.520077    1.290288   -8.582162    2.351424   -4.625915   -2.487808   \n",
       "50%     -2.250535    2.726800   -5.076070    4.127549   -1.507035   -1.463272   \n",
       "75%     -0.436659    4.845798   -2.247442    6.348445    0.231410   -0.491548   \n",
       "max      2.132386   22.057729    2.250210   11.927512    9.880564    6.474115   \n",
       "\n",
       "             PC07        PC08        PC09        PC10  ...        PC21  \\\n",
       "count  315.000000  315.000000  315.000000  315.000000  ...  315.000000   \n",
       "mean    -5.440562    0.048049   -2.585855   -5.688388  ...    0.916146   \n",
       "std      7.111707    7.379311    2.567025    4.879710  ...    4.573313   \n",
       "min    -43.557242  -41.044261  -13.434066  -24.588262  ...  -22.797604   \n",
       "25%     -7.723720   -0.114607   -3.928330   -7.550001  ...    0.131593   \n",
       "50%     -2.986028    0.643230   -2.099049   -4.592390  ...    0.587728   \n",
       "75%     -1.202573    1.717880   -0.770367   -2.733474  ...    1.193791   \n",
       "max      5.431271   17.903574    3.353525    4.031435  ...   27.202839   \n",
       "\n",
       "             PC22        PC23        PC24        PC25        PC26        PC27  \\\n",
       "count  315.000000  315.000000  315.000000  315.000000  315.000000  315.000000   \n",
       "mean    -0.005732    0.120946   -0.130766    0.030085    0.085179    0.217511   \n",
       "std      1.696558    1.131018    0.507588    0.712773    0.489928    1.363315   \n",
       "min     -8.887017   -4.908301   -1.386745   -2.419446   -1.149923   -7.263482   \n",
       "25%     -0.518950   -0.320403   -0.468859   -0.332491   -0.246697   -0.004432   \n",
       "50%      0.083353   -0.048155   -0.145493    0.050678    0.048935    0.421300   \n",
       "75%      0.615344    0.319270    0.256022    0.402808    0.430788    0.856969   \n",
       "max      8.361985    5.466230    1.091435    2.024388    2.745261    2.352333   \n",
       "\n",
       "             PC28       Amount  Fraud  \n",
       "count  315.000000   315.000000  315.0  \n",
       "mean     0.082284   110.947016    1.0  \n",
       "std      0.567332   254.978960    0.0  \n",
       "min     -1.869290     0.000000    1.0  \n",
       "25%     -0.086931     1.000000    1.0  \n",
       "50%      0.160769     6.990000    1.0  \n",
       "75%      0.397953    99.990000    1.0  \n",
       "max      1.521218  2125.870000    1.0  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = fraud_train[fraud_train[\"Fraud\"] == 1]\n",
    "c.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on my observation, the dataset contains 54276 samples with 29 features and it is focused on detecting fraudulent transactions. The features include principal component analysis components that are labeled as PC1 through PC28, which likely represent reduced dimensions of the original transaction data to capture key patterns while minimizing noise. This dimensionality reduction is useful in financial transactions where specific variables may not be immediately interpretable.\n",
    "\n",
    "The target variable is labeled as `Fraud` which indicates whether a transaction is fraudulent 1 or not 0. With only 315 out of 54,276 transactions flagged as fraudulent, the distribution of this variable reveals a significant class imbalance since it represents just 0.58% of the data. This imbalance poses various challenges for model training as typical classification models may perform poorly on the minority class unless appropriate balancing techniques are applied. Another important feature is the transaction `Amount`, which reflects the monetary value of each transaction. Fraudulent transactions have a higher mean amount of 110.94 compared to non fraudulent transactions of 88.06. It suggests that fraudulent transactions tend to involve higher values which could guide model development as larger transaction amounts might be a potential indicator of fraudulent activity. However, the standard deviation is also high for both classes; it demonstrates considerable variability in transaction amounts across both fraudulent and non fraudulent cases. Furthermore, the dataset has no missing values across any of its features as verified by the absence of True values in the isna().any() output. This  allows us for more straightforward analysis and modeling because there is no eed for additional imputation or data cleaning process.\n",
    "\n",
    "As one can see, the combination of principal component analysis components, transaction amount, and class imbalance highlights the dataset's potential for machine learning applications aimed at fraud detection through the insights that are derived from these features. Therefore, these feature could be crucial for developing robust models capable of accurately identifying fraudulent transactions while minimizing false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualizations\n",
    "fraud_train_df = fraud_train.copy()\n",
    "fraud_train_df['color_encoded'] = fraud_train_df['Fraud']\n",
    "fraud_train_df_with_color = fraud_train_df.drop(columns=['Fraud'])\n",
    "correlation_with_color = fraud_train_df_with_color.corr()\n",
    "correlations = correlation_with_color['color_encoded']\n",
    "sorted_correlations = correlations.sort_values(ascending=False)\n",
    "top_n = 10\n",
    "top_positive_correlations = sorted_correlations[sorted_correlations > 0].head(top_n) \n",
    "top_negative_correlations = sorted_correlations[sorted_correlations < 0].tail(top_n) \n",
    "most_correlated_genes = pd.concat([top_positive_correlations, top_negative_correlations])\n",
    "filtered_correlation_matrix = correlation_with_color.loc[most_correlated_genes.index, ['color_encoded']]\n",
    "plt.figure(figsize=(8, 10))\n",
    "sns.heatmap(filtered_correlation_matrix, annot=True, cmap='coolwarm', cbar=True, annot_kws={\"size\": 8})\n",
    "plt.title('Top Correlated Transactions with Fraud Detection', fontsize=14)\n",
    "plt.xlabel('Fraud Detection', fontsize=12)\n",
    "plt.ylabel('Transactions', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap visualization extremely useful information about the correlation between principal component features and the detection of fraudulent transactions. This heatmap illustrates the top correlated components with correlation values that are color coded to highlight the strength and direction of each feature’s relationship with fraud detection. For instance, PC11 and PC04 display the strongest positive correlations with values of 0.28 and 0.24 respectively. These high values suggest that higher values in these components are usually associated with a higher likelihood of fraud. Conversely, components like PC14 and PC17 indicate negative correlations with values of -0.5 and -0.48. These low values show that lower values in these components might be more characteristic of fraudulent transactions.\n",
    "\n",
    "As one can see, the encoded color feature illustrates a perfect correlation, which represents a categorical distinction within the data but may not directly relate to fraud detection. By visualizing these correlations, we can gain valuable insights into the underlying structure of transaction data in order to determine which components have notable associations with fraud. This information is extremely helpful for refining fraud detection algorithms since it allows analysts to focus on components with stronger correlations and potentially reduce noise from less influential components. Therefore, this heatmap provides a clear visual guide to the principal components most relevant to identifying fraudulent activity which helps the development of targeted and efficient fraud detection strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data for ML\n",
    "numeric_features = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "categorical_features = X_train.select_dtypes(include=[\"object\", \"category\"]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_preprocessor = Pipeline(\n",
    "    steps=[\n",
    "        (\"MedianImputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"Standardize\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "categorical_preprocessor = Pipeline(\n",
    "    steps=[\n",
    "        (\"ModalImputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"OneHotEncoder\", OneHotEncoder(handle_unknown=\"infrequent_if_exist\", max_categories=5)),\n",
    "    ]\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"CategoricalPreprocessing\", categorical_preprocessor, categorical_features),\n",
    "        (\"NumericProcessing\", numeric_preprocessor, numeric_features),\n",
    "    ]\n",
    ")\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"Preprocessor\", preprocessor),\n",
    "        (\"classifier\", RandomForestClassifier()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models\n",
    "weights_list = [\n",
    "    {0: 1, 1: 1},\n",
    "    {0: 1, 1: 2},\n",
    "    \"balanced\",\n",
    "]\n",
    "scoring = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"recall\": make_scorer(recall_score),\n",
    "    \"precision\": make_scorer(precision_score, zero_division=0),\n",
    "    \"f1\": make_scorer(fbeta_score, beta=3.5),\n",
    "}\n",
    "rf_param_grid = {\n",
    "    \"classifier__n_estimators\": [50, 100],\n",
    "    \"classifier__max_depth\": [5, 10],\n",
    "    \"classifier__class_weight\": weights_list,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metric_scores(grid, metric):\n",
    "    cv_results = grid.cv_results_\n",
    "    best_index = grid.best_index_\n",
    "    mean_score = cv_results[f\"mean_test_{metric}\"][best_index]\n",
    "    std_score = cv_results[f\"std_test_{metric}\"][best_index]\n",
    "    print(f\"CV {metric} (mean ± std): {mean_score:.3f} ± {std_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;Preprocessor&#x27;,\n",
       "                                        ColumnTransformer(transformers=[(&#x27;CategoricalPreprocessing&#x27;,\n",
       "                                                                         Pipeline(steps=[(&#x27;ModalImputer&#x27;,\n",
       "                                                                                          SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                                                         (&#x27;OneHotEncoder&#x27;,\n",
       "                                                                                          OneHotEncoder(handle_unknown=&#x27;infrequent_if_exist&#x27;,\n",
       "                                                                                                        max_categories=5))]),\n",
       "                                                                         Index([], dtype=&#x27;object&#x27;)),\n",
       "                                                                        (&#x27;NumericProcessing&#x27;,\n",
       "                                                                         Pipeline(steps=[(&#x27;Med...\n",
       "                         &#x27;classifier__max_depth&#x27;: [5, 10],\n",
       "                         &#x27;classifier__n_estimators&#x27;: [50, 100]},\n",
       "             refit=&#x27;f1&#x27;,\n",
       "             scoring={&#x27;accuracy&#x27;: make_scorer(accuracy_score, response_method=&#x27;predict&#x27;),\n",
       "                      &#x27;f1&#x27;: make_scorer(fbeta_score, response_method=&#x27;predict&#x27;, beta=3.5),\n",
       "                      &#x27;precision&#x27;: make_scorer(precision_score, response_method=&#x27;predict&#x27;, zero_division=0),\n",
       "                      &#x27;recall&#x27;: make_scorer(recall_score, response_method=&#x27;predict&#x27;)})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;Preprocessor&#x27;,\n",
       "                                        ColumnTransformer(transformers=[(&#x27;CategoricalPreprocessing&#x27;,\n",
       "                                                                         Pipeline(steps=[(&#x27;ModalImputer&#x27;,\n",
       "                                                                                          SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                                                         (&#x27;OneHotEncoder&#x27;,\n",
       "                                                                                          OneHotEncoder(handle_unknown=&#x27;infrequent_if_exist&#x27;,\n",
       "                                                                                                        max_categories=5))]),\n",
       "                                                                         Index([], dtype=&#x27;object&#x27;)),\n",
       "                                                                        (&#x27;NumericProcessing&#x27;,\n",
       "                                                                         Pipeline(steps=[(&#x27;Med...\n",
       "                         &#x27;classifier__max_depth&#x27;: [5, 10],\n",
       "                         &#x27;classifier__n_estimators&#x27;: [50, 100]},\n",
       "             refit=&#x27;f1&#x27;,\n",
       "             scoring={&#x27;accuracy&#x27;: make_scorer(accuracy_score, response_method=&#x27;predict&#x27;),\n",
       "                      &#x27;f1&#x27;: make_scorer(fbeta_score, response_method=&#x27;predict&#x27;, beta=3.5),\n",
       "                      &#x27;precision&#x27;: make_scorer(precision_score, response_method=&#x27;predict&#x27;, zero_division=0),\n",
       "                      &#x27;recall&#x27;: make_scorer(recall_score, response_method=&#x27;predict&#x27;)})</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">best_estimator_: Pipeline</label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;Preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;CategoricalPreprocessing&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;ModalImputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                                  (&#x27;OneHotEncoder&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;infrequent_if_exist&#x27;,\n",
       "                                                                                 max_categories=5))]),\n",
       "                                                  Index([], dtype=&#x27;object&#x27;)),\n",
       "                                                 (&#x27;NumericProcessing&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;MedianImputer&#x27;,\n",
       "                                                                   SimpleImputer(st...\n",
       "                                                                  (&#x27;Standardize&#x27;,\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  Index([&#x27;PC01&#x27;, &#x27;PC02&#x27;, &#x27;PC03&#x27;, &#x27;PC04&#x27;, &#x27;PC05&#x27;, &#x27;PC06&#x27;, &#x27;PC07&#x27;, &#x27;PC08&#x27;, &#x27;PC09&#x27;,\n",
       "       &#x27;PC10&#x27;, &#x27;PC11&#x27;, &#x27;PC12&#x27;, &#x27;PC13&#x27;, &#x27;PC14&#x27;, &#x27;PC15&#x27;, &#x27;PC16&#x27;, &#x27;PC17&#x27;, &#x27;PC18&#x27;,\n",
       "       &#x27;PC19&#x27;, &#x27;PC20&#x27;, &#x27;PC21&#x27;, &#x27;PC22&#x27;, &#x27;PC23&#x27;, &#x27;PC24&#x27;, &#x27;PC25&#x27;, &#x27;PC26&#x27;, &#x27;PC27&#x27;,\n",
       "       &#x27;PC28&#x27;, &#x27;Amount&#x27;],\n",
       "      dtype=&#x27;object&#x27;))])),\n",
       "                (&#x27;classifier&#x27;,\n",
       "                 RandomForestClassifier(class_weight=&#x27;balanced&#x27;, max_depth=5,\n",
       "                                        n_estimators=50))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;Preprocessor: ColumnTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for Preprocessor: ColumnTransformer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>ColumnTransformer(transformers=[(&#x27;CategoricalPreprocessing&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;ModalImputer&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                 (&#x27;OneHotEncoder&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;infrequent_if_exist&#x27;,\n",
       "                                                                max_categories=5))]),\n",
       "                                 Index([], dtype=&#x27;object&#x27;)),\n",
       "                                (&#x27;NumericProcessing&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;MedianImputer&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;median&#x27;)),\n",
       "                                                 (&#x27;Standardize&#x27;,\n",
       "                                                  StandardScaler())]),\n",
       "                                 Index([&#x27;PC01&#x27;, &#x27;PC02&#x27;, &#x27;PC03&#x27;, &#x27;PC04&#x27;, &#x27;PC05&#x27;, &#x27;PC06&#x27;, &#x27;PC07&#x27;, &#x27;PC08&#x27;, &#x27;PC09&#x27;,\n",
       "       &#x27;PC10&#x27;, &#x27;PC11&#x27;, &#x27;PC12&#x27;, &#x27;PC13&#x27;, &#x27;PC14&#x27;, &#x27;PC15&#x27;, &#x27;PC16&#x27;, &#x27;PC17&#x27;, &#x27;PC18&#x27;,\n",
       "       &#x27;PC19&#x27;, &#x27;PC20&#x27;, &#x27;PC21&#x27;, &#x27;PC22&#x27;, &#x27;PC23&#x27;, &#x27;PC24&#x27;, &#x27;PC25&#x27;, &#x27;PC26&#x27;, &#x27;PC27&#x27;,\n",
       "       &#x27;PC28&#x27;, &#x27;Amount&#x27;],\n",
       "      dtype=&#x27;object&#x27;))])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">CategoricalPreprocessing</label><div class=\"sk-toggleable__content fitted\"><pre>Index([], dtype=&#x27;object&#x27;)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>OneHotEncoder(handle_unknown=&#x27;infrequent_if_exist&#x27;, max_categories=5)</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">NumericProcessing</label><div class=\"sk-toggleable__content fitted\"><pre>Index([&#x27;PC01&#x27;, &#x27;PC02&#x27;, &#x27;PC03&#x27;, &#x27;PC04&#x27;, &#x27;PC05&#x27;, &#x27;PC06&#x27;, &#x27;PC07&#x27;, &#x27;PC08&#x27;, &#x27;PC09&#x27;,\n",
       "       &#x27;PC10&#x27;, &#x27;PC11&#x27;, &#x27;PC12&#x27;, &#x27;PC13&#x27;, &#x27;PC14&#x27;, &#x27;PC15&#x27;, &#x27;PC16&#x27;, &#x27;PC17&#x27;, &#x27;PC18&#x27;,\n",
       "       &#x27;PC19&#x27;, &#x27;PC20&#x27;, &#x27;PC21&#x27;, &#x27;PC22&#x27;, &#x27;PC23&#x27;, &#x27;PC24&#x27;, &#x27;PC25&#x27;, &#x27;PC26&#x27;, &#x27;PC27&#x27;,\n",
       "       &#x27;PC28&#x27;, &#x27;Amount&#x27;],\n",
       "      dtype=&#x27;object&#x27;)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, max_depth=5, n_estimators=50)</pre></div> </div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('Preprocessor',\n",
       "                                        ColumnTransformer(transformers=[('CategoricalPreprocessing',\n",
       "                                                                         Pipeline(steps=[('ModalImputer',\n",
       "                                                                                          SimpleImputer(strategy='most_frequent')),\n",
       "                                                                                         ('OneHotEncoder',\n",
       "                                                                                          OneHotEncoder(handle_unknown='infrequent_if_exist',\n",
       "                                                                                                        max_categories=5))]),\n",
       "                                                                         Index([], dtype='object')),\n",
       "                                                                        ('NumericProcessing',\n",
       "                                                                         Pipeline(steps=[('Med...\n",
       "                         'classifier__max_depth': [5, 10],\n",
       "                         'classifier__n_estimators': [50, 100]},\n",
       "             refit='f1',\n",
       "             scoring={'accuracy': make_scorer(accuracy_score, response_method='predict'),\n",
       "                      'f1': make_scorer(fbeta_score, response_method='predict', beta=3.5),\n",
       "                      'precision': make_scorer(precision_score, response_method='predict', zero_division=0),\n",
       "                      'recall': make_scorer(recall_score, response_method='predict')})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_grid = GridSearchCV(pipeline, rf_param_grid, cv=5, scoring=scoring, refit=\"f1\")\n",
    "rf_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to develop an effective model to detect fraudulent credit card transactions based on their characteristics, we implemented a RandomForestClassifier in our predictive modeling process. Our primary objective was to identify a model that optimally balances false positives and false negatives in order to minimize financial losses due to fraud without unnecessary declines.\n",
    "\n",
    "We began by preprocessing the dataset, which included features such as transaction amount, location, time, and other relevant attributes available at the time of the transaction. Categorical variables were encoded using one-hot encoding to convert them into a numerical format suitable for the model. Numerical features were standardized to ensure that the scale of the data did not bias the model's performance. Missing values were handled through appropriate imputation methods to maintain data integrity and ensure the robustness of the model.\n",
    "\n",
    "To optimize the performance of the RandomForestClassifier, we constructed a pipeline that integrated the preprocessing steps with the classifier. We conducted an extensive grid search over a range of hyperparameters to tune the model. Specifically, we varied the number of estimators of n_estimators between 50 and 100 to determine the optimal number of decision trees in the forest. We also explored maximum depths of max_depth between 5 and 10 to control the complexity of the individual trees and prevent overfitting. In addition, we experimented with different class weighting schemes to address the class imbalance between genuine and fraudulent transactions. The class weights tested included {0:1, 1:1}, {0:1, 1:2}, and balanced. By using the balanced option to adjust weights that are inversely proportional to class frequencies, it makes the model more sensitive to the minority class, which is crucial in fraud detection where fraudulent transactions are much rarer than genuine ones. Hence, the use of balanced class weighting effectively addresses the issue of class imbalance inherent in fraud detection datasets. By adjusting the weights inversely proportional to class frequencies, the model becomes more sensitive to the minority class, which improves its ability to detect fraud without excessively increasing false positives. Moreover, we utilized GridSearchCV with 5 fold cross validation to evaluate the combinations of hyperparameters. The model was evaluated using multiple scoring metrics to capture different aspects of performance, including accuracy, precision, recall, and the F beta score with beta set to 3.5 to emphasize recall over precision.\n",
    "\n",
    "As one can see, the RandomForestClassifier with the identified hyperparameters provides the best performance for our fraud detection needs. It achieves a high recall rate while maintaining reasonable precision; it perfectly aligns with our goal of minimizing financial losses due to fraud and reducing the impact of false positives on customer experience. The selected model demonstrates a strong ability to detect fraudulent transactions on account of the high recall rate of 0.8481012658227848; it means that the model correctly identifies 0.8481012658227848 of all fraudulent transactions, which significantly reduces the risk of undetected fraud. The precision of 0.7613636363636364 indicates that the rate is acceptable given the critical need to catch as many fraudulent activities as possible even though some genuine transactions may be incorrectly flagged as fraudulent. Thus, this model will serve as the foundation for our automated fraud detection system which enables real time identification of suspicious transactions and allows for prompt action to prevent potential losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters found with cross-validation:\n",
      "{'classifier__class_weight': 'balanced',\n",
      " 'classifier__max_depth': 5,\n",
      " 'classifier__n_estimators': 50}\n",
      "\n",
      "CV accuracy (mean ± std): 0.998 ± 0.001\n",
      "CV precision (mean ± std): 0.790 ± 0.080\n",
      "CV recall (mean ± std): 0.857 ± 0.033\n",
      "CV f1 (mean ± std): 0.851 ± 0.029\n",
      "\n",
      "Test Accuracy: 0.9975679858500995\n",
      "Test Precision: 0.7613636363636364\n",
      "Test Recall: 0.8481012658227848\n",
      "Test F1 Score: 0.8023952095808383\n"
     ]
    }
   ],
   "source": [
    "# report model metrics\n",
    "# print the best parameters and cross-validation metrics\n",
    "print(\"\")\n",
    "print(f\"Best parameters found with cross-validation:\")\n",
    "pprint(rf_grid.best_params_)\n",
    "print(\"\")\n",
    "print_metric_scores(rf_grid, \"accuracy\")\n",
    "print_metric_scores(rf_grid, \"precision\")\n",
    "print_metric_scores(rf_grid, \"recall\")\n",
    "print_metric_scores(rf_grid, \"f1\")\n",
    "\n",
    "# make predictions on the test set using the best model\n",
    "y_pred = rf_grid.predict(X_test)\n",
    "\n",
    "# calculate test metrics\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "test_precision = precision_score(y_test, y_pred)\n",
    "test_recall = recall_score(y_test, y_pred)\n",
    "test_f1 = fbeta_score(y_test, y_pred, beta=1)\n",
    "\n",
    "# print test metrics\n",
    "print(\"\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test Precision: {test_precision}\")\n",
    "print(f\"Test Recall: {test_recall}\")\n",
    "print(f\"Test F1 Score: {test_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['credit-fraud.joblib']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "dump(rf_grid, \"credit-fraud.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model and utilizing cross validation, the grid search identified the optimal hyperparameters for our fraud detection model as a RandomForestClassifier with 50 estimators in n_estimators, a maximum depth of 5 in max_depth, and a class weight set to balanced in class_weight. By using the RandomForestClassifier with these optimized parameters, our model effectively distinguishes between fraudulent and genuine credit card transactions based on measurable attributes, which significantly enhances our ability to detect fraud in real time and aligns with our goal of minimizing financial losses while reducing inconvenience to legitimate customers.\n",
    "\n",
    "As a result, we trained the final RandomForestClassifier model using these optimized hyperparameters. This model achieves a cross validation accuracy of 0.998 ± 0.001, a cross validation precision of 0.790 ± 0.080, a cross validation recall of 0.857 ± 0.033, and a cross validation F-beta score with beta = 3.5 of 0.851 ± 0.029. Hence, these results indicate that the model is highly effective at correctly identifying fraudulent transactions while maintaining a reasonable rate of false positives. We then evaluated its performance on the test dataset and obtained a test accuracy of 0.9975679858500995, a test precision of 0.7613636363636364, a test recall of 0.8481012658227848, and a test F1 score with beta = 3.5 of 0.8023952095808383. The high test accuracy indicates that the model correctly classifies the vast majority of transactions. Additionally, the high test recall of 0.8481012658227848 demonstrates the model's strong ability to identify a significant proportion of fraudulent transactions, which is crucial for reducing undetected fraud. The test precision of 0.7613636363636364 suggests that this rate is acceptable given our emphasis on recall in fraud detection despite the fact that  some legitimate transactions may be incorrectly flagged as fraudulent. Although balancing precision and recall is extremely important, recall is often prioritized in the context of fraud detection in order to ensure that as many fraudulent transactions as possible are caught. Even though it means that some of the customers have to deal with some false alarms, this approach minimizes financial losses due to fraud while maintaining an acceptable level of customer satisfaction. The use of balanced class weighting effectively addresses the issue of class imbalance inherent in fraud detection datasets where fraudulent transactions are much rarer than genuine ones. By adjusting the weights inversely proportional to class frequencies, the model becomes more sensitive to detecting fraud without being biased towards predicting the majority class, which results in a model with high predictive power that can detect fraudulent transactions that might be overlooked in certain situations.\n",
    "\n",
    "As one can see, the RandomForestClassifier with the optimized hyperparameters outperforms other models we considered for this task. Its ability to minimize false negatives on the test dataset indicates superior performance in identifying fraudulent transactions based on measurable attributes. Thus, this model effectively supports our goal of developing an automated fraud detection system by providing accurate and timely assessments of transaction legitimacy. By deploying this model, we can promptly flag suspicious transactions in order to reduce the impact of fraud and enhance customer experience in the near future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to develope an automated fraud detection system for credit card transactions, it is crucial to determine whether precision or recall is more important in terms of model's performance with the bank's objectives. Precision and recall are both vital metrics in classification tasks but one can be more useful depending on the specific application and the consequences of false positives and false negatives.\n",
    "\n",
    "Precision measures the proportion of transactions identified as fraudulent that are actually fraudulent. A high precision means that when the model flags a transaction as fraudulent, it is likely correct; it minimizes the number of false positives, which are legitimate transactions incorrectly labeled as fraud. False positives can lead to customer dissatisfaction as genuine transactions may be declined or require additional verification which could cause inconvenience to customers. On the other hand, recall measures the proportion of actual fraudulent transactions that the model correctly identifies. A high recall indicates that the model is effective at catching most fraudulent activities which minimizes false negatives. False negatives occur when fraudulent transactions are not detected by the model; it allows fraud to proceed unchecked, which can result in significant financial losses for the bank and its customers as well as damage to the bank's reputation.\n",
    "\n",
    "Given the banking institution's primary goal of minimizing financial losses due to fraud, recall is generally more important in this context because false negatives are known as the undetected fraudulent transactions which can have severe financial implications and erode customer trust. By prioritizing recall, the model aims to identify as many fraudulent transactions as possible even though it allows the model to accept a higher number of false positives. However, focusing on recall without considering precision can lead to an excessive number of false positives because it can inconvenience customers by flagging legitimate transactions as fraudulent and lead to declined transactions or unnecessary security checks. These inconvenience can ruin the customer experience. Therefore, precision cannot be ignored while we prioritize recall. Thus, balancing precision and recall is essential since model needs to achieve a high recall to catch the majority of fraudulent transactions as well as maintaining a reasonable precision to avoid overwhelming customers and fraud analysts with false alarms. In the performance metrics obtained, the model achieved a recall of 0.8481012658227848 and a precision of 0.7613636363636364. This indicates that there is a trade off with the number of false positives generated when the model is effective at detecting fraud.\n",
    "\n",
    "Considering the bank's emphasis on loss minimization and the fact that fraud can lead to substantial financial and reputational damage, recall plays an very important role in this scenario since cost of false negative which is missing a fraudulent transaction is typically higher than the cost of false positive which is incorrectly flagging a legitimate transaction. Therefore, fraudulent transactions could result in direct financial losses and lead to additional costs associated with fraud investigation and recovery efforts. In contrast, false positives cost customer inconvenience but the impact is generally less severe than that of false negatives. By implementing efficient customer communication strategies and verification processes, it can mitigate the negative effects of false positives. For example, sending immediate alerts to customers to confirm suspicious transactions can resolve issues quickly without significant disruption.\n",
    "\n",
    "As one can see, recall is more critical in this context due to the high stakes associated with undetected fraudulent transactions even though both precision and recall are important for an effective fraud detection model. Hence, the model should prioritize identifying as many fraudulent activities as possible to minimize financial losses and protect both the bank and its customers. Therefore, we should improve precision to enhance customer satisfaction and reduce unnecessary operational overhead caused by false positives in order to achieve an optimal balance between the two metrics in our fraud detection system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our comprehensive analysis and the performance metrics achieved, we conclude that the RandomForestClassifier model with the optimized hyperparameters is suitable for deployment in our fraud detection system because model demonstrates exceptional performance by achieving high accuracy, recall, and acceptable precision on both cross validation and test datasets. This model achieves a cross validation accuracy of 0.998 ± 0.001, a cross validation precision of 0.790 ± 0.080, a cross validation recall of 0.857 ± 0.033, and a cross validation F-beta score with beta = 3.5 of 0.851 ± 0.029. On the test dataset, it obtains an accuracy of 0.9975679858500995, a precision of 0.7613636363636364, a recall of 0.8481012658227848, and an F-beta score of 0.8023952095808383. Hence, these results indicate that the model is highly effective at correctly identifying fraudulent transactions while maintaining a reasonable rate of false positives.\n",
    "\n",
    "In the context of fraud detection, recall is of paramount importance due to the significant financial and reputational risks associated with undetected fraudulent transactions since high recall of 0.8481012658227848 on the test dataset means that the model successfully identifies the majority of fraudulent activities, which minimizes potential losses and enhancing the security of our services and aligns with our primary goal of reducing false negatives to prevent fraud from going unnoticed.\n",
    "\n",
    "While precision is slightly lower than recall, it remains acceptable at 0.7613636363636364, which indicates that most transactions flagged as fraudulent are indeed fraudulent even though some legitimate transactions may be incorrectly identified as false positives. Given the critical need to detect as much fraud as possible, this trade off is reasonable since the impact of false positives can be mitigated through efficient customer communication and verification processes to minimize customer inconvenience. Considering the model's outstanding performance metrics and the emphasis on recall for fraud detection, we recommend using this model in practice. By deploying the RandomForestClassifier with the optimized hyperparameters, it improves our real time fraud detection capabilities by allowing us to promptly flag suspicious transactions for further review or immediate action. Thus, this proactive approach will help safeguard the bank's assets and protect our customers from financial losses due to fraudulent activities. Furthermore, the model's high accuracy of 0.9975679858500995 ensures that it correctly classifies the majority of transactions correctly. The balance between high recall and acceptable precision makes it a valuable tool for our loss minimization efforts. By effectively detecting fraudulent transactions and reducing the incidence of false negatives, we can strengthen our fraud prevention framework and maintain customer trust.\n",
    "\n",
    "As one can see, the RandomForestClassifier model meets our performance expectations and strategic objectives because it significantly reduces undetected fraud while keeping false positives at a manageable level. We recommend implementing this model in our fraud detection system and establishing continuous monitoring and periodic retraining to adapt to evolving fraud patterns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
